\chapter{Method}

\begin{enumerate}   \item Description of Node Copying as a method
  \begin{enumerate}     \item Node structure expansion
    \begin{enumerate}       \item Back pointers
      \item Modifications \item Bounds on numbers of the above \item How it
      works \item Why it works     \end{enumerate}
    \item Surrounding structure (collection of version information)
    \begin{enumerate}       \item Considerations about effecient storage and
    retrieval of version info     \end{enumerate}
  \end{enumerate}
  \item Description of ``Rollback'' as a method \begin{enumerate}     \item
  Description of reordering and optimization of operations (see algorithm
  \ref{alg:opsort} on page \pageref{alg:opsort})
  \end{enumerate}
  \item Definitions of various application profiles (how the DS is used)
  \end{enumerate}

\section{The Node Copying method}
Node Copying is a method described in \cite{Driscoll198986} by which a data
structure may be made partially persistent through the systematic expansion of
the nodes of the data structure. An auxillary data structure maintaining entry
points into the data structure, such as which node is the head of a linked list,
is also introduced. These two modifications of the data structure enable queries
to be made with $O(1)$ factor running time overhead and $O(1)$ space
consumption, given that the original data structure has an upper bounded
in-degree. In the following, I will describe in detail the general procedure as
well as giving the concrete example of the linked list.

\subsection{Node structure expansion}
As part of the Node Copying method, the node structure is expanded by adding two
arrays; one for ``back pointers'' and one for ``modifications''.

\begin{description}
  \item[Back pointers] 

  point back from a node $x$ to every node $y_i$ which points to $x$. When the
  in-degree has an upper bound, it is possible to restrict the size of the back
  pointers array. Whenever a node $y_i$ changes one of its pointers, the node
  $x$ it previously pointed to has the corresponding back pointer cleared, and
  the node $z$ it now points to has its corresponding back pointer set to point
  to $y_i$.

  \item[Modifications] store updates made to any of the fields of the node,
  including non-pointer fields, made since the node was created -- that is, the
  original field values from the time of construction remain unchanged. A
  modification record consist of a version number a field identifier and a
  value.

  When a field is to be modified on a node whose modifications array is already
  full, a copy of the node is constructed using the latest version of each field
  (including the one which is to be changed), thus resulting in a new node with
  an empty modifications array. The back pointers are also copied from the
  original node, and if the field being updated is a pointer field, the
  corresponding back pointer is updated in the destination node. This procedure
  is why the overall method is called Node Copying. A constant maximum number of
  modifications records is chosen such that node copying will have an amortized
  cost of $O(1)$.
\end{description}

\subsubsection{Amortized cost of Node Copying}
\todo[inline]{Write up proof of amortized time cost of node copying}

\section{Rollback}
Rollback is an approach to persistence based on the techniques described in
\cite{Tsotras1995237}, namely the combination of the na\"ive ``copy'' and
``log'' methods.
\subsection{The na\"ive approaches}
The ``copy'' approach makes a full copy of every version of the data structure
and makes it available by direct indexing to achieve $O(1)$ access overhead
factor. Creating each copy becomes more expensive in time and space the more
elements are inserted. In the worst case, when only insertions and no deletions
or modifications are made, the cost of creating $n$ versions is
$O\left(\binom{n+1}{2}\right)$.

The ``log'' approach conserves space by recording for each change made to the
data structure just enough information necessary to undo or redo it, thus giving
a space overhead factor per operation of $O(1)$. A ``current'' version
$v_{current}$ of the data structure is maintained. Given $v_{current}$, the
version $v_x$ can be produced by undoing or redoing all the changes between
$v_{current}$ and $v_x$ depending on which is the oldest. As the number of
versions $n$ increases, accessing a specific version becomes potentially more
costly. In the worst case, the overhead factor is $O(n)$ when $v_{current}$ is
$v_0$ and $v_x$ is $v_n$, or opposite.

\subsection{The hybrid approach}
An obvious hybrid of the two na\"ive approaches is to keep the records of each
operation like in the ``log'' approach, and storing a full copy like in the
``copy'' approach only at certain intervals. To access version $v_x$, the
nearest version to $v_x$ of which there exists a full copy, $v_s$, is retrieved.
Then, using the log of the versions from $v_s$ to $v_x$, the data structure
corresponding to $v_x$ is produced and returned.

Naturally, a user parameter is necessary to control the intervals at which full
copies are made. Let this parameter be $d$ and the number of versions $n$. There
will then be $\left\lfloor \frac{n}{d} \right\rfloor$ full copies, and the
maximum number of undos or redos to reach a specific version is then
$O(k+\frac{d}{2})$, where $k$ is the time required to retrieve the full copy. It
is now easy to see that the greater $d$ is, the fewer full copies will be made,
and thus the space cost is reduced accordingly. Likewise, the distance to the
version in the middle between two full copies increases with $d$, inducing a
higher cost for producing that version.

\todo[inline]{Justify its comparison to Node Copying?}

\subsubsection{Adaptive copy interval increase}
It might be preferrable to be able to specify a space usage limit rather than a
fixed interval distance. When the space limit is near, $d$ is increased by a
factor $c$, and the existing full copies are discarded save every $c$ copy, thus
freeing up space for additional full copies.

\todo[inline]{Insert figure illustrating adaptive copy interval increase.}

If there are costs involved with deleting a copy of the data structure, the
interval increase will cause the version in which it is performed to cost more
than usual versions. When analyzed, the cost can be amortized over the other
versions, leading to an overhead of $O(1)$.

\todo[inline]{Write proof using potential method.}

\subsection{Operations batch optimization}
For certain underlying data structures, it may prove to be feasible to
pre-process the batch of operations between a full copy $v_s$ and the requested
version $v_x$ in order to reduce the actual work necessary to reach $v_x$.

Two different types of pre-processing can be made to reduce work: Eliminating
superfluous operations and reordering operations.

\subsubsection{Eliminating operations}
The batch of operations between $v_s$ and $v_x$ may contain some operations
which will not need to be explicitly applied. The cases are:
\begin{enumerate}
  \item An \textsc{insert} operation followed by a \textsc{remove} operation at
  the same effective index -- both can be removed from the batch since they
  cancel each other out.
  \label{item:elop-insert-remove}

  \item An \textsc{insert} operation followed by a \textsc{modify} operation at
  the same effective index -- the former can have its associated data value
  changed to that of the latter, and the latter can be removed from the batch.
  \label{item:elop-insert-modify}

  \item A \textsc{modify} operation followed by a \textsc{remove} operation at
  the same effective index -- the former can be removed from the batch.
  \label{item:elop-modify-remove}
\end{enumerate}

Cases \ref{item:elop-insert-remove} and \ref{item:elop-insert-modify} may of
course be combined in the sequence of one \textsc{insert} operation, one or more
\textsc{modify} operations and finally a \textsc{remove} operataion.

An algorithm implementing the above elimination cases would have to consider all
operations in the batch and properly determine if the requirements of any of the
cases are satisfied.

The general idea of the algorithm is to identify cases
\ref{item:elop-insert-remove}, \ref{item:elop-insert-modify} and
\ref{item:elop-modify-remove} and make the necessary maintenance before removing
the relevant operations.

In case \ref{item:elop-insert-remove}, the operations between the
\textsc{insert} operation and the \textsc{remove} operation are iterated to
adjust the index on those operations which operate on indices greater than the
index of the inserted element in the corresponding version.

In case \ref{item:elop-insert-modify}, the \textsc{insert} operation gets the
data value from the \textsc{modify} operation prior to the removal of the
latter, and no further maintenance is necessary.

In case \ref{item:elop-modify-remove}, the \textsc{modify} operation can simply
be removed since it will have no net effect.

Pseudo-code for such an algorithm is found in \ref{alg:eliminate-ops}.

\begin{algorithm}[p]
  \caption{An algorithm for eliminating superfluous operations}
  \label{alg:eliminate-ops}
  \begin{algorithmic}
    \Procedure{EliminateSuperfluousOps}{Batch $B$}
      \For {each operation $o_i \in B$, from last to first}
        \If {\textsc{type}($o_i$) $\in \left\{\textsc{insert},\textsc{modify}\right\}$}
        \State $c\gets\textsc{index}(o_i)$
          \For {each operation $o_j \in \left\{B|j>i\right\}$}
            \Switch {$\textsc{type}(o_j)$}
              \Case{\textsc{insert}}
                \If {$\textsc{index}(o_j) \le c$}
                  \State $c \gets c+1$
                \EndIf
              \EndCase
              \Case{\textsc{modify}}
                \If {$\textsc{index}(o_j) = c$}
                  \State $\textsc{data}(o_i) \gets \textsc{data}(o_j)$
                  \State remove $o_j$ from $B$
                \EndIf
              \EndCase
              \Case{\textsc{remove}}
                \If {$\textsc{index}(o_j) < c$}
                  \State $c \gets c-1$
                \ElsIf {$\textsc{index}(o_j) = c$}
                  \State $c \gets \textsc{index}(o_i)$
                  \Switch {$\textsc{type}(o_i)$}
                    \Case {\textsc{insert}}
                      \For {each operation $o_k \in \left\{B|i<k<j\right\}$}
                        \If {$\textsc{index}(o_k) > c$}
                          \State $\textsc{index}(o_k) \gets \textsc{index}(o_k) - 1$
                        \Else
                          \Switch {\textsc{type}($o_k$)}
                            \Case {\textsc{insert}}
                              \If {$\textsc{index}(o_k) \le c$}
                                \State $c \gets c+1$
                              \EndIf
                            \EndCase
                            \Case {\textsc{remove}}
                              \If {$\textsc{index}(o_k) \le c$}
                                \State $c \gets c-1$
                              \EndIf
                            \EndCase
                          \EndSwitch
                        \EndIf
                      \EndFor
                      \State remove $o_j$ from $B$
                      \State remove $o_i$ from $B$
                    \EndCase
                    \Case {\textsc{modify}}
                      \State remove $o_i$ from $B$
                    \EndCase
                  \EndSwitch
                \EndIf
              \EndCase
            \EndSwitch
          \EndFor
        \EndIf
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
 
\subsubsection{Reordering operations}

If some operations are potentially  expensive to perform on the underlying data
structure, such as inserting  inside a linked list, the operations can be
reordered to allow the to be  carried out in a more efficient way. With the
example of a linked list, it is  expensive to carry out the operations in an
isolated fashion since every  operation would have to start from the head of the
list. If instead the  operations are reordered such that they can be applied in
sequence with  non-decreasing index, the head of the list need only be accessed
once.


\todo[inline]{Mention that it requires more knowledge of underlying DS. Prove
that the algorithm is correct.}

See pseudo-code for the reordering in algorithm \ref{alg:opsort}.

\begin{algorithm}[!ht]
  \caption{Algorithm for sorting a set of operations}
  \label{alg:opsort}
  \begin{algorithmic}
    \Function {SortOperations}{set of operations $O$}
      \State result $\gets$ empty set
      \While {$O$ not empty}
        \State // Find left-most operation with minimum index of application
        \State $o_{min} \gets$ op. with minimum index of application
        \State $index_{min} \gets$ index in $O$ of $o_{min}$
        \State \textsc{remove} $o_{min}$ from $O$
        \Statex
        \State // Compensate operations which will now be after $o_{min}$ instead of before
        \If {$o_{min}$ is an \textsc{insert} op.}
          \For {each op. $o_i,o_i\in \left\{O | 0\le i<index_{min}\right\}$ }
            \State \textsc{index}($o_i$) $\gets$ \textsc{index}($o_i$)-1
          \EndFor
        \ElsIf {$o_{min}$ is a \textsc{remove} op.}
          \For {each op. $o_i,o_i\in \left\{O | 0\le i<index_{min}\right\}$}
            \State \textsc{index}($o_i$) $\gets$ \textsc{index}($o_i$)+1
          \EndFor
        \EndIf
        \State \textsc{append} $o_{min}$ to result
      \EndWhile
      \Statex
      \State \Return result
    \EndFunction
  \end{algorithmic}
\end{algorithm}
