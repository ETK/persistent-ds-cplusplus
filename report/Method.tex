\chapter{Method}

\begin{enumerate}   \item Description of Node Copying as a method
  \begin{enumerate}     \item Node structure expansion
    \begin{enumerate}       \item Back pointers
      \item Modifications \item Bounds on numbers of the above \item How it
      works \item Why it works     \end{enumerate}
    \item Surrounding structure (collection of version information)
    \begin{enumerate}       \item Considerations about effecient storage and
    retrieval of version info     \end{enumerate}
  \end{enumerate}
  \item Description of ``Rollback'' as a method \begin{enumerate}     \item
  Description of reordering and optimization of operations (see algorithm
  \ref{alg:opsort} on page \pageref{alg:opsort})
  \end{enumerate}
  \item Definitions of various application profiles (how the DS is used)
  \end{enumerate}

\section{The Node Copying method}
Node Copying is a method described in \cite{Driscoll198986} by which a data
structure may be made partially persistent through the systematic expansion of
the nodes of the data structure. An auxillary data structure maintaining entry
points into the data structure, such as which node is the head of a linked list,
is also introduced. These two modifications of the data structure enable queries
to be made with $O(1)$ factor running time overhead and $O(1)$ space
consumption, given that the original data structure has an upper bounded
in-degree. In the following, I will describe in detail the general procedure as
well as giving the concrete example of the linked list.

\subsection{Node structure expansion}
As part of the Node Copying method, the node structure is expanded by adding two
arrays; one for ``back pointers'' and one for ``modifications''.

\begin{description}
  \item[Back pointers] 

  point back from a node $x$ to every node $y_i$ which points to $x$. When the
  in-degree has an upper bound, it is possible to restrict the size of the back
  pointers array. Whenever a node $y_i$ changes one of its pointers, the node
  $x$ it previously pointed to has the corresponding back pointer cleared, and
  the node $z$ it now points to has its corresponding back pointer set to point
  to $y_i$.

  \item[Modifications] store updates made to any of the fields of the node,
  including non-pointer fields, made since the node was created -- that is, the
  original field values from the time of construction remain unchanged. A
  modification record consist of a version number a field identifier and a
  value.

  When a field is to be modified on a node whose modifications array is already
  full, a copy of the node is constructed using the latest version of each field
  (including the one which is to be changed), thus resulting in a new node with
  an empty modifications array. The back pointers are also copied from the
  original node, and if the field being updated is a pointer field, the
  corresponding back pointer is updated in the destination node. This procedure
  is why the overall method is called Node Copying. A constant maximum number of
  modifications records is chosen such that node copying will have an amortized
  cost of $O(1)$.
\end{description}

\subsubsection{Amortized cost of Node Copying}
\todo[inline]{Write up proof of amortized time cost of node copying}

\section{Rollback}
Rollback is an approach to persistence based on the techniques described in
\cite{Tsotras1995237}, namely the combination of the na\"ive ``copy'' and
``log'' methods.
\subsection{The na\"ive approaches}
The ``copy'' approach makes a full copy of every version of the data structure
and makes it available by direct indexing to achieve $O(1)$ access overhead
factor. Creating each copy becomes more expensive in time and space the more
elements are inserted. In the worst case, when only insertions and no deletions
or modifications are made, the cost of creating $n$ versions is
$O\left(\binom{n+1}{2}\right)$.

The ``log'' approach conserves space by recording for each change made to the
data structure just enough information necessary to undo or redo it, thus giving
a space overhead factor per operation of $O(1)$. A ``current'' version
$v_{current}$ of the data structure is maintained. Given $v_{current}$, the
version $v_x$ can be produced by undoing or redoing all the changes between
$v_{current}$ and $v_x$ depending on which is the oldest. As the number of
versions $n$ increases, accessing a specific version becomes potentially more
costly. In the worst case, the overhead factor is $O(n)$ when $v_{current}$ is
$v_0$ and $v_x$ is $v_n$, or opposite.

\subsection{The hybrid approach}
An obvious hybrid of the two na\"ive approaches is to keep the records of each
operation like in the ``log'' approach, and storing a full copy like in the
``copy'' approach only at certain intervals. To access version $v_x$, the
nearest version to $v_x$ of which there exists a full copy, $v_s$, is retrieved.
Then, using the log of the versions from $v_s$ to $v_x$, the data structure
corresponding to $v_x$ is produced and returned.

Naturally, a user parameter is necessary to control the intervals at which full
copies are made. Let this parameter be $d$ and the number of versions $n$. There
will then be $\left\lfloor \frac{n}{d} \right\rfloor$ full copies, and the
maximum number of undos or redos to reach a specific version is then
$O(k+\frac{d}{2})$, where $k$ is the time required to retrieve the full copy. It
is now easy to see that the greater $d$ is, the fewer full copies will be made,
and thus the space cost is reduced accordingly. Likewise, the distance to the
version in the middle between two full copies increases with $d$, inducing a
higher cost for producing that version.

\todo[inline]{Justify its comparison to Node Copying?}

\subsubsection{Adaptive copy interval increase}
It might be preferrable to be able to specify a space usage limit rather than a
fixed interval distance. When the space limit is near, $d$ is increased by a
factor $c$, and the existing full copies are discarded save every $c$ copy, thus
freeing up space for additional full copies.

\todo[inline]{Insert figure illustrating adaptive copy interval increase.}

If there are costs involved with deleting a copy of the data structure, the
interval increase will cause the version in which it is performed to cost more
than usual versions. When analyzed, the cost can be amortized over the other
versions, leading to an overhead of $O(1)$.

\todo[inline]{Write proof using potential method.}

\subsection{Operations batch optimization}
For certain underlying data structures, it may prove to be feasible to
pre-process the batch of operations between a full copy $v_s$ and the requested
version $v_x$ in order to reduce the actual work necessary to reach $v_x$.

Two different types of pre-processing can be made to reduce work:

\begin{description}

  \item[Eliminating operations] If an element is first inserted by one operation
  and since removed by another, both operations can be removed from the batch.
  Likewise, if an element is inserted with one value and a later operation
  modifies it, those two operations can be reduced to one.

  \item[Reordering operations] If some operations are potentially expensive to
  perform on the underlying data structure, such as inserting inside a linked
  list, the operations can be reordered to allow the to be carried out in a more
  efficient way. With the example of a linked list, it is expensive to carry out
  the operations in an isolated fashion since every operation would have to
  start from the head of the list. If instead the operations are reordered such
  that they can be applied in sequence with non-decreasing index, the head of
  the list need only be accessed once.
\end{description}


\todo[inline]{Mention that it requires more knowledge of underlying DS. Prove
that the algorithm is correct.}

See pseudo-code for the reordering in algorithm \ref{alg:opsort}.

\begin{algorithm}[!h]
  \caption{Algorithm for sorting a set of operations}
  \label{alg:opsort}
  \begin{algorithmic}
    \Function {SortOperations}{set of operations $O$}
      \State result $\gets$ empty set
      \While {$O$ not empty}
        \State // Find left-most operation with minimum index of application
        \State $o_{min} \gets$ op. with minimum index of application
        \State $index_{min} \gets$ index in $O$ of $o_{min}$
        \State \textsc{remove} $o_{min}$ from $O$
        \Statex
        \State // Compensate operations which will now be after $o_{min}$ instead of before
        \If {$o_{min}$ is an \textsc{insert} op.}
          \For {each op. $o_i,o_i\in \left\{O | 0\le i<index_{min}\right\}$ }
            \State \textsc{index}($o_i$) $\gets$ \textsc{index}($o_i$)-1
          \EndFor
        \ElsIf {$o_{min}$ is a \textsc{remove} op.}
          \For {each op. $o_i,o_i\in \left\{O | 0\le i<index_{min}\right\}$}
            \State \textsc{index}($o_i$) $\gets$ \textsc{index}($o_i$)+1
          \EndFor
        \EndIf
        \State \textsc{append} $o_{min}$ to result
      \EndWhile
      \Statex
      \State \Return result
    \EndFunction
  \end{algorithmic}
\end{algorithm}
