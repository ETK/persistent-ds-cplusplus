\chapter{Empirical Analysis}
\label{chapter:empirical-analysis}

In this chapter I will analyse and discuss the experiments I have implemented
and run to examine the performance of the different approaches in practice. The
purpose of the empirical analysis is to make qualified recommendations as to
which of the approaches to use in different usage scenarios.

\section{Implementation}
The implementations are programmed in C++ and conform to the C++11 standard. For
compiling the program, the CMake 2.8.9 system has been used together with the
GNU C++ compiler.

The source code of the experiment program should be available together with this
document. It should also be obtainable from the following URL:

{\small\url{https://github.com/sirlatrom/persistent-ds-cplusplus/archive/master.zip}}

In the abstract class defined in ``\texttt{AbstractDoublyLinkedList.h}'' are
declared the common functions that the different implementations should provide
to the main experiment program, which is defined in ``\texttt{main.cpp}''.

Compiling is done by changing into the ``\texttt{build}'' directory, running
``\texttt{cmake ..}'' and then ``\texttt{make}''.

\subsection{Execution environment}
The empirical analysis is based on the output of executing different
implementations on a machine with the following specifications:

% \begin{table}[!ht]
\begin{tabular}{|l|l|}
\hline
CPU & Intel\textregistered Core\texttrademark i5-2400 CPU @ 3.10GHz $\times$ 4
\\
\hline
Memory & Hynix/Hyundai 2048 MB DDR3 RAM @ 1333 MHz $\times$ 2 \\
\hline
Operating System & Ubuntu 12.10 64-bit \\
\hline
\end{tabular}
%\end{table}

The machine has been put into single-user mode prior to execution, and no other
non-operating system processes have run at the same time apart from the login
shell.

\subsection{Implemented operations}

I have implemented Node Copying and Rollback for a doubly linked list. The
Rollback implementation exists in two variants:

\begin{description}

  \item[Blackbox] Uses a simple, ephemeral doubly linked list for applying each
   operation between full copy and destination version separately.

  \item[Eliminate-Reorder] Eliminates superfluous operations using Algorithm
  \ref{alg:eliminate-ops} and then reorders the remaining ones using Algorithm
  \ref{alg:opsort} before applying them in a single iteration through the
  underlying ephemeral doubly linked list.

\end{description}

Both variants default to a maximum of 4000 full copies and an initial distance
of 65 operations between them. Whenever 4000 full copies have been made, every
second one of them is deleted (i.e. the number of full copies then becomes
2000), and the distance is doubled. These figures have been chosen by bisecting
within the two-dimensional between them to find the most efficient values for
the execution environment. Both variants employ the technique described in
Section \ref{subsec:the-hybrid-approach} for moving the full copies within
$\pm\frac{d}{2}$ of their original position.

All implementations support the following operations:

\begin{description}

  \item[$\textsc{insert}(i,d)$] Inserts an element with data $d$ at index $i$.

  \item[$\textsc{modify}(i,d)$] Modifies the data of the element at index $i$ to
  $d$.

  \item[$\textsc{remove}(i)$] Removes the element at index $i$.

\end{description}

These first three represent the usual operations available on a linked list,
with bulk operation friendly parameters (conventional linked list
implementations take a pointer to a node instead of an index). The difference is
that they create a new version of the data structure.

\begin{description}

  \item[$\textsc{head}(v)$] Returns the head node of the list at version
  $v$.

  \item[$\textsc{size}(v)$] Returns the size of the list at version $v$.

\end{description}

These next two are also usually available in a linked list implementation, but
these variants take a version number $v$ from which to return the information.

\begin{description}

  \item[$\textsc{access}(v,i)$] Returns the value of the element at index $i$ in
  version $v$.

  \item[$\textsc{num\_versions}()$] Returns the total number of versions.

\end{description}

These last two are implemented for the convenience of testing bulk usage of the
data structure.

Let the operations $\textsc{insert}(i,d)$, $\textsc{modify}(i,d)$,
$\textsc{remove}(i)$ and $\textsc{access}(v,i)$ be the ones benchmarked and
henceforth be referred to as ``the operations''.

\subsection{Implemented usage scenarios}

In benchmarking the performance of the approaches in practice, we will look at
how they perform under various usage scenarios. We cosider only scenarios
concerning at least a sizeable total number of operations (i.e. $N \ge 1000$).

When either of the approaches are to be used in practice, one can imagine
different usage scenarios:

\begin{description}

  \item[Random] The operations are executed in random order with no particular
  pattern, except that if an operation is illegal (such as removing when the
  list is empty), an \textsc{insert} operation is chosen. They may be weighted
  such that there is different probability for choosing different operations.

  In my experiments, the probability is equal (25\%) for \textsc{insert},
  \textsc{modify}, \textsc{remove} and \textsc{access} operations. Thus, the
  number of operations of each type will be close to $\frac{1}{4}$ of the total
  number of operations.

  \item[Sequential] The different types of operations are executed in sequences.

  In my experiments, equally many \textsc{insert}, \textsc{modify},
  \textsc{remove} and \textsc{access} operations are made, and in that order.
  Thus, the number of operations of each type will be exactly a quarter of the
  total number of operations. The index on which the operations operate on are
  still randomly chosen unless otherwise specified.

\end{description}

Other usage scenarios could be imagined, including specially tailored worst-case
scenarios designed to stress the implementations to their fullest, and scenarios
simulating how specific algorithms solving well-known problems, such as planar
point location, would behave. However, due to time constraints, such scenarios
have been left out.

\subsection{Program executable arguments}

The above scenarios have been implemented in the program \texttt{msc}, which
accepts the following arguments:

\begin{description}

  \item[\texttt{-\@{}-count}/\texttt{-c \{num\}}] Total number of operations to
  carry out (default: 1000).

  \item[\texttt{-\@{}-randomize-operations}] If passed, applies the operations
  as described in the Random scenario above -- otherwise as in the Sequential
  secenario (default: off).

  \item[\texttt{-\@{}-rollback-eliminate-reorder}/\texttt{-l}] Will use the
  Eliminate-Reorder Rollback implementation (default).

  \item[\texttt{-\@{}-rollback-blackbox}/\texttt{-r}] Will use the Blackbox
  Rollback implementation.

  \item[\texttt{-\@{}-node-copying}/\texttt{-p}] Will use the Node Copying
  implementation.

  \item[\texttt{-\@{}-max-snapshot-dist}/\texttt{-d \{num\}}] Maximum number of
  operations between full copies (default: 65, applies only to the Rollback
  implementations).

  \item[\texttt{-\@{}-max-num-snapshots}/\texttt{-m \{num\}}] Maximum number of
  full copies before adaptive fallback is carried out (default: 4000, applies
  only to the Rollback implementations).

  \item[\texttt{-\@{}-head-only}/\texttt{-h}] If set, \textsc{access} operations
  will work on index 0, i.e. the head of the list, instead of a randomly chosen
  index.

  \item[\texttt{-\@{}-store-results}/\texttt{-s}] If set, will store results in
  an SQLite database file ``sqlite.db''.
  
\end{description}

Only the last one specified of the arguments \texttt{-p}, \texttt{-l} and
\texttt{-r} is used.

\section{Time measurements}
\label{sec:time-results}

I have run a series of experiments with various combinations of program
arguments in order to determine the time-related performance of each
implementation. The same arguments are used 10 times for each of the following
total operation counts, which are exponentially spaced between 1000 and 2000000:

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
1000 & 2327 & 5415 & 12599 & 29317 & 68219 & 158740 & 369375 & 859506 & 2000000\\
\hline
\end{tabular}

Experiments simulating the Sequential scenario are not run for the last three
counts, since with the Rollback implementations they would exceed the available
memory of the execution environment, and with the Node Copying implementation
they would take exceedingly long to complete.

If the \texttt{-\@{}-head-only} argument is given, the index passed to the
\textsc{access} operations is 0. Otherwise, the index is randomly selected from
the range $[0..N[$ where $N$ is the number of elements in the list at the
version in question. Effectively, the Eliminate-Reorder implementation should
perform worse than the Blackbox when this argument is given, since there is no
reordering to be done when all operations work on index 0.

In the following graphs, all data points are averages over 10 runs with
identical parameters, each representing the duration spent on the respective
operations.  For the Rollback variants, it includes fetching the relevant full
copy, pre-processing the sequence of operations, and producing and navigating
the resulting list. For Node Copying, it includes looking up which persistent
node is the head at the version in question as well as navigating to the desired
index. The Y error bars indicate $\pm 1.96$ times the standard deviation, i.e. a
95\% confidence interval. Data points for the Sequential scenario are marked by
boxes, while those for the Random scenario are marked by diamonds. Please note
that, unless otherwise stated, the count axis indicates the count of the
operation type being discussed, and \emph{not} the total operation count.

\subsection{Access}
For this operation, the time measured is that which it takes to get the head of
the list of a random version and then iterating to a randomly chosen index
within that list.

\subsubsection{Random scenario}
The following conclusions regarding the Random scenario are evident when
examining the diamond-shaped data points plotted in Figure
\ref{fig:access-duration-per-op-vs-count.pdf}:


\textit{Node Copying is faster than either of the Rollback implementations
regardless of \texttt{count}.} This is most likely due to the constants being
lower for Node Copying than for Rollback when accessing the head of a version,
and the cost eventually increasing for the Rollback variants.

\textit{The Blackbox variant of the Rollback implementation shows to be faster
than the Eliminate-Reorder variant.} This is most likely due to the list never
growing very large, owing to the \textsc{remove} operations occurring with 25\%
probability mixed between other operations, shrinking the list. When the list is
not very long, it is cheaper to iterate the list from the head to carry out the
operations rather than running $O(n^2)$ time algorithms on the operation
sequences.

\textit{The Rollback implementations both show an trend to increase toward the
higher end of the count axis}, which can be explained by the fact that the
distance between full copies will double after operation no.
$4000\times65 = 2.6\times10^5$ and again after operation no.
$2.6\times10^5+130\times2000=5.2\times10^5$ changing operations.

When these events occur, some time is spent on discarding full copies, and
afterwards it will take twice as long in expectation to reach a uniformly
randomly chosen version.

\begin{figure}[!htb]
  \center
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/compare_rollbacks/access-duration-per-op-vs-count.pdf}

  \caption{Results for \textsc{access} operations.}

  \label{fig:access-duration-per-op-vs-count.pdf}
\end{figure}

\subsubsection{Sequential scenario}
The following conclusions regarding the Sequential scenario are evident when
examining the box-shaped data points plotted in Figure
\ref{fig:access-duration-per-op-vs-count.pdf}:

\textit{Up to total operation counts of about 5415 corresponding to approx.
$\frac{3}{4}5415 \approx 4061$ versions, Node Copying is faster than Rollback.}
But since longer sequences of \textsc{insert} operations are carried out with
higher total operation counts, the average length of the list at a randomly
chosen version increases.

Once the head of a given version has been obtained, Node Copying is slower at
iterating through the list than Rollback. With large enough data sets, the list
will be so long that the cost of iterating the list becomes greater, when
compared to Rollback, than the benefit of being faster at getting the head node.

If the Blackbox variant of Rollback could allocate enough memory to complete a
greater number of operations, it would likely surpass Node Copying in speed per
\textsc{access} operation around the $10^5$ mark.

\textit{The time spent on optimizing the operations sequence in the
Eliminate-Reorder variant makes it perform worse at \textsc{access} operations
than the Blackbox variant with low operation counts.} Once enough elements are
inserted, it pays off to reorder the operations for a single iteration through
the list instead of $n$ potentially full iterations for $n$ operations.

This is the case already at a total operations count of 2327, which corresponds
to a list length of at least 582 elements, where Eliminate-Reorder becomes
faster than Blackbox. With slightly more than 5415 total operations,
corresponding to a list length of at least 1354, it is also faster than Node
Copying.

\subsection{Insert}
For this operation, the time measured is that which it takes to insert an
element at a randomly chosen index of the list in its most recent version.

\subsubsection{Random scenario}
The following conclusions regarding the Random scenario are evident when
examining the diamond-shaped data points plotted in Figure
\ref{fig:insert-duration-per-op-vs-count.pdf}:

\textit{The Blackbox variant of Rollback dominates the other implementations
with any operation count.} Eliminate-Reorder starts out the slowest, but after
5415 total operations, corresponding to at least 1354 \textsc{insert}
operations, it becomes faster than Node Copying. Eventually, Eliminate-Reorder
and Blackbox are very nearly equally fast.

The simple explanation to this result is that Node Copying is slower than the
Rollback implementations at iterating through the list to find the point of
insertion. Once enough versions exist with long enough lists, this deficiency
will cost Node Copying more than it gains from its fast retrieval of the head
node.

\begin{figure}[!htbp]
  \center
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/compare_rollbacks/insert-duration-per-op-vs-count.pdf}

  \caption{Results for \textsc{insert} operations.}

  \label{fig:insert-duration-per-op-vs-count.pdf}
\end{figure}

\subsubsection{Sequential scenario}
The following conclusions regarding the Sequential scenario are evident when
examining the box-shaped data points plotted in Figure
\ref{fig:insert-duration-per-op-vs-count.pdf}. Note that prior to the
\texttt{count} \textsc{access} operations, equally many \textsc{insert},
\textsc{modify} and \textsc{remove} operations have been executed, and thus a
total of $1+\frac{3}{4}$\texttt{count} versions exist.

\textit{The Rollback variants are virtually equally fast.}

This is because, in contrast to the Random scenario, the most recent version is
already available when the \textsc{insert} operation is to be applied, and thus
that version does not need to be retrieved before the operation can be applied.

\textit{Node Copying is slower than both Rollback variants, and increasingly
more so as more \textsc{insert} operations are carried out.} This result shows
most clearly how, as pointed out before, Node Copying is slower at iterating
through the list to the point where the changing operation is supposed to take
place.
% The actual insertion is also more expensive with Node Copying, since it does
% more than just record the index and value and update pointers.

\subsection{Other operations}
The \textsc{modify} and \textsc{remove} operations show virtually the same
results as the \textsc{insert} operation, since the outcomes mostly depend on
how long the list gets in the versions created --- a figure which depends
primarily on the number of \textsc{insert} operations.

The graphs are nevertheless included in Figure
\ref{fig:modify-remove-duration-per-op-vs-count.pdf} for comparison.

\begin{figure}[!htbp]
  \center
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/compare_rollbacks/modify-duration-per-op-vs-count.pdf}
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/compare_rollbacks/remove-duration-per-op-vs-count.pdf}

  \caption{Results for \textsc{modify} and \textsc{remove} operations.}

  \label{fig:modify-remove-duration-per-op-vs-count.pdf}
\end{figure}

\subsubsection{Total duration}
When looking at the total duration, i.e. the time from start to finish of the
entire scenario, it turns out that Node Copying is the fastest --- see Figure
\ref{fig:total-duration-per-op-vs-count.pdf} --- but this depends largely on the
chosen probabilities/ratios of each operation.

\begin{figure}[!htbp]
  \center
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/total-duration-vs-count-avg.pdf}
  \includegraphics[width=0.8\textwidth]%
  {figures/graphs/total-duration-per-op-vs-count-avg.pdf}

  \caption{Total duration and duration per operation across all the operations
  of one experiment for each batch size.}

  \label{fig:total-duration-per-op-vs-count.pdf}
\end{figure}

\section{Space estimates}
In order to estimate the memory usage of the different implementations,
preprocessor directives have been introduced which control whether time or
measurements are made. If the \texttt{MEASURE\_SPACE} symbol is defined, no
lines of code which measure the time of operations are compiled or executed.
Instead, code lines are introduced which estimate the memory usage.

The memory usage for an instance of the Rollback implementation with $F$ full
copies and $N$ total operations is estimated according to the following formula:
\begin{eqnarray*}
\textnormal{total\_space} &=& \textsc{size\_of}(\textnormal{ephemeral node})
\times
\sum_{i=1}^F {\textsc{size}(\textnormal{full copy}_i)}\\
&&+N\times\textsc{size\_of}(\textnormal{operation record})\\
&&+F\times\textsc{size\_of}(\textnormal{full copy record})\\
&&+ \textsc{size\_of}(\textnormal{auxillary DS})
\end{eqnarray*}

The total memory reserved by the program when using Eliminate-Reorder for
large data sets is measurably smaller when observed with OS utilites, than when
using Blackbox. This is because fewer nodes are allocated which would be deleted
again as part of getting form version $v_current$ to version $v_x$.

For Node Copying, the estimation is more accurate, given that every time a new
persistent node is created, either due to an \textsc{insert} operation or due to
a copy being made as described in Section \ref{subsec:node-structure-expansion},
a counter is incremented by the size of a persistent node. The size of the
auxillary data structure indicating which node is the head in each version is
also estimated.

In Figure \ref{fig:space-results.pdf} it is clearly visible that, as expected,
Rollback uses significantly more space than Node Copying in the Sequential
scenario --- and consistently more so in the Random scenario.

\begin{figure}[!ht]
  \center \includegraphics[width=0.8\textwidth]%
  {figures/graphs/space-results.pdf}

  \caption{Estimated memory usage.}

  \label{fig:space-results.pdf}
\end{figure}
