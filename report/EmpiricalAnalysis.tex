\chapter{Empirical Analysis}

I have implemented Node Copying and Rollback for a doubly linked list. Both
implementations support the following operations:

\begin{description}

  \item[$\textsc{insert}(i,d)$] Inserts an element with data $d$ at index $i$.

  \item[$\textsc{modify}(i,d)$] Modifies the data of the element at index $i$ to
  $d$.

  \item[$\textsc{remove}(i)$] Removes the element at index $i$.

\end{description}

These first three represent the usual operations available on a linked list,
with bulk operation friendly parameters (conventional linked list
implementations take a pointer to a node instead of an index). The difference is
that they create a new version of the data structure.

\begin{description}

  \item[$\textsc{head}(v)$] Returns the head node of the list at version
  $v$.

  \item[$\textsc{size}(v)$] Returns the size of the list at version $v$.

\end{description}

These next two are also usually available in a linked list implementation, but
these variants take the number of a version $v$ from which to return the
information.

\begin{description}

  \item[$\textsc{access}(v,i)$] Returns the value of the element at index $i$ in
  version $v$.

  \item[$\textsc{num\_versions}()$] Returns the total number of versions.

\end{description}

These last two are implemented for the convenience of testing bulk usage of the
data structure.

Let the operations $\textsc{insert}(i,d)$, $\textsc{modify}(i,d)$,
$\textsc{remove}(i)$ and $\textsc{access}(v,i)$ be the ones benchmarked.

In bemchmarking the performance of the approaches in practice, we will look at
how they perform under various usage scenarios.

When either of the two approaches are to be used in practice, one can imagine
different usage scenarios:

\begin{description}

  \item[Uniformly random] The operations are executed in random order with no
  particular pattern. They may be weighted such that there is different
  probability for choosing different operations.

  \item[Sequential] The different types of operations are executed in sequences,
  e.g. first a number of \textsc{insert} operations, then a number of
  \textsc{access} operations.

  \item[Worst-case] With certain usage patterns, one implementation may perform
  significantly worse than average.

\end{description}

The above scenarios have been implemented in the program \texttt{msc}, which
accepts the following parameters:

\begin{description}

  \item[\texttt{-\@{}-count}/\texttt{-c \{num\}}] Total number of operations to carry
  out (default: 1000).

  \item[\texttt{-\@{}-randomize-operations}] If passed, applies the operations
  chosen at random between the four types. Otherwise, insertions are applied
  first, then modifications, then removals, then access operations (default:
  off).

  \item[\texttt{-\@{}-rollback-reorder-lazy}/\texttt{-l}] Will use the Rollback
  implementation for applying the operations (default).

  \item[\texttt{-\@{}-partiallypersistent}/\texttt{-p}] Will use the Node
  Copying implementation for applying the operations.

  \item[\texttt{-\@{}-max-snapshot-dist}/\texttt{-d \{num\}}] Maximum number of
  operations between snapshots (default: 65, applies only to the Rollback
  implementation).

  \item[\texttt{-\@{}-max-num-snapshots}/\texttt{-m \{num\}}] Maximum number of
  snapshots before adaptive fallback is carried out (default: 2000, applies only to the Rollback
  implementation).

  \item[\texttt{-\@{}-store-results}/\texttt{-s}] If set, will store results in
  an SQLite database file ``sqlite.db''.

\end{description}

\section{Results}

I have run a series of experiments with various combinations of program
arguments in order to determine the performance of each implementation.

The sequential scenario is tested by first running a bulk of \textsc{insert}
operations, then a bulk of \textsc{modify} operations, then a bulk of
\textsc{remove} operations and finally a bulk of \textsc{access} operations. The
bulks are of the same size, i.e. an equal fraction of the \texttt{count}
argument. If the \texttt{-\@{}-head-only} argument is given, the index passed to
the operations is 0. Otherwise, the index is randomly selected from the range
$[0..N[$ where $N$ is the number of elements in the list. As for the
\textsc{access} operations, the version is randomly chosen between the versions
which exist when the operation is run.

\subsection{Execution environment}
The empirical analysis is based on the output of executing different
implementations on a machine with the following specifications:

%\begin{table}[!ht]
\begin{tabular}{|l|l|}
\hline
CPU & Intel\textregistered Core\texttrademark i5-2400 CPU @ 3.10GHz $\times$ 4
\\
\hline
Memory & Hynix/Hyundai 2048 MB DDR3 RAM @ 1333 MHz $\times$ 2 \\
\hline
Operating System & Ubuntu 12.10 64-bit \\
\hline
\end{tabular}
%\end{table}


\begin{figure}[!htb]
  \center
  \includegraphics[width=.8\textwidth]%
  {figures/graphs/access-duration-vs-count-avg.pdf}
  \includegraphics[width=.8\textwidth]%
  {figures/graphs/access-duration-per-op-vs-count-avg.pdf}

  \caption{Comparison of implementations and scenarious for various operation
  counts. Note how the Node Copying method keeps an approximately constant
  overhead factor per operation, whereas the Rollback method eventually hits the
  upper limit for full copies, and access time increases. It is reached faster
  in the sequential scenario since the full copies are larger.}

  \label{fig:access-duration-vs-count-avg}
\end{figure}
